name: Deploy with Helm

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-west-1
  ECR_REPOSITORY: portfolio-app
  EKS_CLUSTER_NAME: portfolio-cluster
  ACCOUNT_ID: "537124942860"

permissions:
  id-token: write
  contents: read

jobs:
  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::537124942860:role/github-actions-role
        aws-region: us-west-1
        audience: sts.amazonaws.com
        role-session-name: GitHubActions

    - name: Install AWS CLI
      run: |
        echo "Installing AWS CLI..."
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip -q awscliv2.zip
        sudo ./aws/install --update
        aws --version

    - name: Verify AWS Authentication
      run: |
        echo "Verifying AWS Authentication..."
        aws sts get-caller-identity
        echo "Listing EKS clusters..."
        aws eks list-clusters

    - name: Get ECR Repository URI
      id: ecr-repo
      run: |
        echo "Getting ECR repository URI..."
        ECR_URI="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPOSITORY}"
        echo "ECR URI: $ECR_URI"
        echo "ecr_uri=$ECR_URI" >> $GITHUB_OUTPUT

    - name: Login to Amazon ECR
      run: |
        echo "Logging in to Amazon ECR..."
        aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com
        echo "ECR login successful"

    - name: Setup kubectl and Helm
      run: |
        echo "Installing kubectl..."
        curl -LO "https://dl.k8s.io/release/v1.28.1/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        kubectl version --client
        
        echo "Installing Helm..."
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh
        helm version
        
        # Install AWS EKS kubectl auth plugin
        echo "Installing EKS kubectl auth plugin..."
        curl -LO "https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.11/aws-iam-authenticator_0.6.11_linux_amd64"
        chmod +x ./aws-iam-authenticator_0.6.11_linux_amd64
        sudo mv ./aws-iam-authenticator_0.6.11_linux_amd64 /usr/local/bin/aws-iam-authenticator
        aws-iam-authenticator version

    - name: Configure kube credentials with temp AWS credentials
      run: |
        echo "Setting up Kubernetes configuration..."
        
        # Using AWS CLI to update kubeconfig with the AWS IAM authenticator
        aws eks update-kubeconfig --name ${EKS_CLUSTER_NAME} --region ${AWS_REGION} --auth-type token
        
        # Verify identity being used
        echo "AWS Identity being used:"
        aws sts get-caller-identity
        
        # Test connection
        echo "Testing Kubernetes connection..."
        kubectl cluster-info

    - name: Setup aws-auth ConfigMap
      run: |
        echo "Setting up aws-auth ConfigMap..."
        
        # Get current AWS identity information
        ROLE_ARN=$(aws sts get-caller-identity --query "Arn" --output text)
        echo "Current Role ARN: $ROLE_ARN"
        
        # Check if the ConfigMap exists
        if kubectl get configmap aws-auth -n kube-system &> /dev/null; then
          echo "aws-auth ConfigMap exists, updating..."
          # Get the existing ConfigMap
          kubectl get configmap aws-auth -n kube-system -o yaml > aws-auth-existing.yaml
          
          # Extract current mapRoles (if any)
          grep -A 999 "mapRoles:" aws-auth-existing.yaml > current-map-roles.txt || echo "No existing mapRoles found"
          
          # Create a new ConfigMap with both roles
          cat > aws-auth-cm.yaml << EOF
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: aws-auth
          namespace: kube-system
        data:
          mapRoles: |
            - rolearn: arn:aws:iam::${ACCOUNT_ID}:role/github-actions-role
              username: github-actions
              groups:
                - system:masters
            - rolearn: $ROLE_ARN
              username: github-actions-session
              groups:
                - system:masters
        EOF
          
          # Apply the updated ConfigMap
          kubectl apply -f aws-auth-cm.yaml
        else
          echo "aws-auth ConfigMap does not exist, creating..."
          
          # Create a new ConfigMap from scratch
          cat > aws-auth-cm.yaml << EOF
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: aws-auth
          namespace: kube-system
        data:
          mapRoles: |
            - rolearn: arn:aws:iam::${ACCOUNT_ID}:role/github-actions-role
              username: github-actions
              groups:
                - system:masters
            - rolearn: $ROLE_ARN
              username: github-actions-session
              groups:
                - system:masters
        EOF
          
          # Apply the new ConfigMap
          kubectl apply -f aws-auth-cm.yaml
        fi
        
        # Wait for ConfigMap to propagate
        echo "Waiting for aws-auth ConfigMap to propagate..."
        sleep 15
        
        # Verify connection now
        echo "Verifying Kubernetes access after ConfigMap update..."
        kubectl get nodes || echo "Could not access cluster nodes yet. Continuing..."

    - name: Deploy with Helm
      env:
        IMAGE_TAG: ${{ github.sha }}
      run: |
        echo "=========== DEPLOYMENT INFO ==========="
        echo "Image tag: $IMAGE_TAG"
        echo "Using namespace: prod"
        echo "======================================"
        
        # Create namespace
        kubectl create namespace prod --dry-run=client -o yaml | kubectl apply -f -
        
        # Debug info before Helm
        echo "Current directory: $(pwd)"
        ls -la ./kubernetes/helm/app || echo "Helm chart directory not found"
        
        # Get ECR Repository URI
        ECR_URI="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPOSITORY}"
        echo "Full image reference: ${ECR_URI}:${IMAGE_TAG}"
        
        # Before Helm deployment, test cluster connectivity again
        echo "Testing Kubernetes connection before Helm deployment:"
        kubectl cluster-info
        
        # Deploy with Helm
        helm upgrade --install portfolio-app ./kubernetes/helm/app \
          --set image.repository=${ECR_URI} \
          --set image.tag=${IMAGE_TAG} \
          --namespace prod \
          --create-namespace
        
        # Show resources
        kubectl get pods -n prod --ignore-not-found=true || true
        kubectl get services -n prod --ignore-not-found=true || true

    - name: Notify Slack
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        fields: repo,message,commit,author,action,eventName,ref,workflow,job,took
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: always()